\documentclass{beamer}

\input{preamble.tex}

\title{OpenMP for Computational Scientists}
\subtitle{5: Programming your GPU with OpenMP}

\begin{document}

\frame{\titlepage}

%-------------------------------------------------------------------------------
\begin{frame}
\frametitle{Outline}
\begin{itemize}
  \item GPU introduction
  \item The OpenMP \mintinline{fortran}|target| directive
  \item Memory movement
\end{itemize}
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}
\frametitle{GPU performance}

\begin{columns}
\begin{column}{0.5\textwidth}
\begin{adjustbox}{max width={\textwidth}}
\begin{tikzpicture}
  \begin{axis}[ybar,
               symbolic x coords={Broadwell, Skylake, P100, V100},
               xtick=data,
               ylabel={Memory Bandwidth (GB/s)}]
    \addplot coordinates {(Broadwell,130) (Skylake,191) (P100,550) (V100,841)};
  \end{axis}
\end{tikzpicture}
\end{adjustbox}
\end{column}

\begin{column}{0.5\textwidth}
\begin{adjustbox}{max width={\textwidth}}
\begin{tikzpicture}
  \begin{axis}[ybar,
               symbolic x coords={Broadwell, Skylake, P100, V100},
               xtick=data,
               ylabel={FP64 GFLOPS/s}]
    \addplot coordinates {(Broadwell,1550) (Skylake,3760) (P100,4700) (V100,7000)};

  \end{axis}
\end{tikzpicture}
\end{adjustbox}
\end{column}
\end{columns}

\begin{itemize}
  \item Volta GPUs offer 4.4X memory bandwidth and 1.9X the FLOPS/s of dual-socket Skylake.
\end{itemize}

\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}
\frametitle{Unlocking this potential}
\begin{itemize}
  \item GPUs made of many cores (SMs):
    \begin{itemize}
      \item V100 has 80 SMs.
      \item P100 has 56 SMs.
    \end{itemize}
  \item Each SM consists of 64 FP32 CUDA cores.
  \item CUDA cores are really organised as 2 vector units 32 wide (called warps).
\end{itemize}

\begin{block}{Take away}
GPUs are really vector-architectures made up of smaller blocks which execute together.
\end{block}
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}
\frametitle{GPUs need lots of parallelism}
\begin{itemize}
  \item GPUs are \emph{throughput optimised}, whereas CPUs are \emph{latency optimised}.
  \item Throughput optimised also called \emph{latency tolerant}.
  \item GPUs achieve this by running many operations at once, and overlapping these with each other.
  \item Hence need many (many) operations\dots
  \item A V100 has 5,120 processing elements, each needing multiple units of work to overlap.
\end{itemize}
\begin{block}{Take away}
Massive amounts of parallelism to exploit.
\end{block}
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{Device model}
\begin{itemize}
  \item OpenMP has a host/device model.
  \item Execution begins on the host CPU, with zero or more devices connected to the host.
  \item Memory spaces \emph{not} shared!
  \item Some data copied automatically, plus explicit copying.
  \item Directives are used to transfer execution to the device.
  \begin{minted}{fortran}
  !$omp target [clause [clause] ...]
  !$omp end target
  \end{minted}
  \item Host executiuon idels until target region completes (exact semantics based on tasks).
\end{itemize}

\vfill

\begin{center}
\begin{tikzpicture}
  \draw (-1,0) node {Host};
  \draw (-1,-1) node {Device};
  \draw (0,0) -- (2,0);
  \draw[dashed] (2,0) -- (3,-1);

  % GPU
  \draw (3,-1) -- (5,-1);
  \draw (3,-1.2) -- (5,-1.2);
  \draw (3,-1.4) -- (5,-1.4);
  \draw (3,-1.6) -- (5,-1.6);
  \draw (3,-0.8) -- (5,-0.8);
  \draw (3,-0.6) -- (5,-0.6);
  \draw (3,-0.4) -- (5,-0.4);

  \draw[dashed] (5,-1) -- (6,0);
  \draw[->] (6,0) -- (8,0);
\end{tikzpicture}
\end{center}
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{The target directive}
\begin{minted}{fortran}
!$omp target [clause [clause]...]
!$omp end target 
\end{minted}
\begin{itemize}
  \item Starts executing \emph{in serial} on the target device.
  \item Need other directives to expand parallelism.
  \item \mintinline{fortran}|nowait| clause:
    \begin{itemize}
      \item Allows host thread to continue working. Must synchronise later using tasks.
    \end{itemize}
  \item Other clauses mainly about memory movement, which we'll come to later.
  \item In general, you'll run loops on the device using:
  \begin{minted}[frame=single]{fortran}
  !$omp target teams distribute parallel do
  do i = 1, N
    ... ! Loop body
  end do
  !$omp end target teams distribute parallel do
  \end{minted}
\end{itemize}
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{Execution model: teams}
\begin{itemize}
  \item OpenMP \emph{threads} on a device are grouped into a \emph{team}.
  \item Can synchronise threads \emph{within} a team. \emph{Cannot} synchronise between teams.
  \item Groups of teams are called a \emph{league}.
  \item \mintinline{fortran}|target| directive offloads (serial) execution to device.
  \item \mintinline{fortran}|teams| directive creates a league of times.
  \item Master thread in each team (redundantly) executes the code.
\end{itemize}

\begin{minted}[frame=single]{fortran}
!$omp target teams 
... ! Code
!$omp end target teams
\end{minted}
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{Execution model: distribute}
\begin{itemize}
  \item Share iterations of a loop between teams.
  \item Each team gets part of the iteration space.
  \item Change default assignment with \mintinline{fortran}|dist_schedule(static)| clause. Optionally include chunk size.
  \item Still only the master thread in the team executes them.
\end{itemize}

\begin{minted}[frame=single]{fortran}
!$omp target teams distribute
do i = 1, N
... ! Code
end do
!$omp end target teams distribute
\end{minted}
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{Execution model: parallel do}
\begin{itemize}
  \item Same semantics as on the CPU!
  \item Launches threads within the team and shares iterations between threads.
  \item Note, iterations that were assigned to the team by \mintinline{fortran}|distribute| directive are shared between threads in the team.
  \item Can use the \mintinline{fortran}|schedule| clause too.
  \item Remember, can synchronise between threads in the team.
\end{itemize}

\begin{minted}[frame=single]{fortran}
!$omp target teams distribute parallel do
do i = 1, N
... ! Code
end do
!$omp end target teams distribute parallel do
\end{minted}
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{Execution example}
\begin{minted}[frame=single]{fortran}
!$omp target teams distribute parallel do
do i = 1, N
  c(i) = a(i) + b(i)
end do
!$omp end target teams distribute parallel do
\end{minted}
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}
\frametitle{Data movement}
\begin{itemize}
  \item Remember: memory is \emph{not} shared between host and target.
  \item Combination of implicit and explicit memory movement.
  \item This is the most complicated part of the offload specification.
  \item Memory movement is often a performance killer.
    \begin{itemize}
      \item A V100 has 900 GB/s peak memory bandwidth.
      \item Connected to the host via PCIe with 32 GB/s peak bandwidth.
      \item Transfers between host and device are relatively very slow: minimise them.
    \end{itemize}
\end{itemize}
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}
\frametitle{Data regions}
\begin{itemize}
  \item Data needs mapping between host and device memory spaces.
  \item Variable names exist in host and device space: the compiler sorts out which one you mean when you use them in your code.
  \item Mapping/transfers occur when
    \begin{itemize}
      \item enter/exit a \mintinline{fortran}|target| region.
      \item \mintinline{fortran}|target enter/exit data| constructs.
      \item \mintinline{fortran}|update| directives.
    \end{itemize}
  \item Default behaviour:
    \begin{itemize}
      \item Scalars are mapped \mintinline{fortran}|firstprivate|.
      \item This means the \emph{do not} get copied back to the host.
      \item Stack arrays are mapped \mintinline{fortran}|tofrom|.
      \item Heap arrays are not mapped by default.
    \end{itemize}
\end{itemize}
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{The map clause}
\begin{itemize}
  \item Specify the transfer of data between host and device on a \mintinline{fortran}|target| region.

  \item On entering the region, copy from host to device:
    \begin{minted}{fortran}
    map(to: A(1:N), x)
    \end{minted}

  \item On exiting the region, copy from device to host. At start of region, these are uninitialised on the device.
    \begin{minted}{fortran}
    map(from: A(1:N), x)
    \end{minted}

  \item Same as applying \mintinline{fortran}|map(to: ...)| and \mintinline{fortran}|map(from: ...)|
    \begin{minted}{fortran}
    map(tofrom: A(1:N), x)
    \end{minted}

  \item Allocate data on the device for use only on the device. It is uninitalised, and can't be copied back to the host.
    \begin{minted}{fortran}
    map(alloc: A(1:N))
    \end{minted}
\end{itemize}
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{Enter/exit data directives}
\begin{itemize}
  \item Often want to perform initial device data environment setup once, run through iteratative loop, copying back at end.
  \item \textbf{Do not} want to copy the data every iteration! Very expensive.
  \item Use \mintinline{fortran}|target enter data| and \mintinline{fortran}|target exit data| constructs to control device data environment.
\end{itemize}

\begin{minted}[frame=single,fontsize=\small]{fortran}
!$omp target enter data map(to: A(1:N), B(1:N), C(1:N))

do t = 1, 1000000
  !$omp target
  ... ! Read A and B, write C
  !$omp end target
end do

!$omp target exit data map(from: C(1:N))

\end{minted}
Bulk transfers happen at beginning and end, not for every \mintinline{fortran}|target| region in the big loop.
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}
\frametitle{Update directive}
\begin{itemize}
  \item Often need to transfer data between host and device between different \mintinline{fortran}|target| regions.
  \item E.g. the host does something between the two regions.
  \item Use the \mintinline{fortran}|update| directive to move the data explicitly between host and device.
  \item Example on next slide\dots
\end{itemize}
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{Update directive}
\begin{minted}[frame=single,fontsize=\small,linenos]{fortran}
!$omp target enter data map(to: A(1:N), B(1:N), C(1:N))
!$omp target
... ! Use A, B and C on device
!$omp end target

! Copy A from device to host
!$omp target update from(A(1:N))

! Change A on the host
A = 1.0

! Copy A from host to device
!$omp target update to(A(1:N))

!$omp target
... ! Use A, B and C on device
!$omp end target

!$omp target exit data map(from: C(1:N))
\end{minted}

\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{Reductions}
\begin{minted}[frame=single,breaklines]{fortran}
integer :: i, N = 1000
real(kind=8), allocatable :: A(:), B(:)
real(kind=8) :: total

!$omp target map(to: A(1:N), B(1:N))
!$omp teams distribute parallel do map(tofrom:total) reduction(+:total)
do i = 1, N
  total = total + (A(i) * B(i))
end do
!$omp end teams distribute parallel do
!$omp end target
\end{minted}

\begin{itemize}
  \item \mintinline{fortran}|total| is a scalar, so by default is \mintinline{fortran}|firstprivate|.
  \item I.e. Each thread on the device gets its own copy.
  \item Importantly, it is not copied back to the host!
  \item You \emph{must} use a \mintinline{fortran}|map| clause to bring the result back.
\end{itemize}

\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}
\frametitle{CUDA Toolkit: NVprof}
\begin{itemize}
  \item The CUDA toolkit works with code written in OpenMP 4.5 without any special configuration.
  \item Useful to use the profiler \mintinline{bash}|nvprof|.
  \item Particularly useful to check it ran on a GPU! Can silently fallback to CPU execution.
  \item Can generate high level profiling information, a timeline, and generate data for NVIDIA's \mintinline{bash}|nvvp| profiler.
\end{itemize}
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{nvprof output}
\begin{minted}{bash}
nvprof ./stencil_target
TODO
\end{minted}
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}
\frametitle{Exercise}
\begin{itemize}
  \item Port your 5-point stencil code to the GPU.
  \item Use the \mintinline{fortran}|target enter/exit data| constructs to transfer data.
  \item Use the \mintinline{fortran}|target teams distribute parallel do| construct for execution.
  \item Print out the grid sum for every iteration:
    \begin{itemize}
      \item Need to use \mintinline{fortran}|reduction| clause.
      \item Remember to \mintinline{fortran}|map| the reduction result!
    \end{itemize}
  \item Extra: Think about the performance compared to your CPU version.
\end{itemize}
\end{frame}
%-------------------------------------------------------------------------------
\end{document}

