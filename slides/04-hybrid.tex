\documentclass{beamer}

\input{preamble.tex}

\title{OpenMP for Computational Scientists}
\subtitle{4: Combining MPI and OpenMP}

\begin{document}

\frame{\titlepage}

%-------------------------------------------------------------------------------
\section{Outline}
\begin{frame}
\frametitle{Outline}
Programming beyond a single multi-core CPU.
\begin{itemize}
  \item Non-uniform Memory Access
  \item Thread affinity in OpenMP
  \item Combining MPI with OpenMP
\end{itemize}
\end{frame}

%-------------------------------------------------------------------------------
\section{Recap}
\begin{frame}
\frametitle{Recap}

We've already come a long way!

\begin{itemize}
  \item Parallelise loops with OpenMP: \mintinline{fortran}|!$omp parallel do|.
  \item Data sharing clauses.
  \item Synchronisation with barriers, atomics and \mintinline{fortran}|critical| regions.
  \item Reductions with the \mintinline{fortran}|reduction| clause.
  \item The cache hierarchy.
  \item Performance analysis and the Roofline model.
  \item Vectorisation along with the OpenMP \mintinline{fortran}|simd| construct.
  \item Optimisations for memory access.
\end{itemize}

\end{frame}

%-------------------------------------------------------------------------------
\section{NUMA}
\begin{frame}
\frametitle{NUMA Architecture}
\begin{itemize}
  \item In reality on a dual-socket system, each socket is physically connected to half of the memory.
  \item A core in the first socket wanted memory attached to the other socket must:
    \begin{itemize}
      \item Go via the socket-to-socket interconnect.
      \item Access memory via the other socket's memory controllers.
    \end{itemize}
  \item Accessing memory from other socket is slower than access from own socket.
\end{itemize}
\begin{center}
\begin{tikzpicture}
  % Draw 4 cores for socket 0
  \foreach \i in {0,1,3,4} {
    \foreach \j in {0, 1} {
      \draw (\i,\j) rectangle (\i+1,\j+1);
    }
  }

  % Draw large memory
  \draw (-0.5,3) rectangle (2.3,4);
  \draw (2.7,3) rectangle (5.5,4);
  \draw[dashed] (-0.7,2.8) rectangle (5.7,4.2);

  % Connect sockets to memory
  \draw (1,2) -- (1,3);
  \draw (4,2) -- (4,3);
  \draw[dashed] (2,1) -- (3,1); % QPI

\end{tikzpicture}
\end{center}
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}
\frametitle{Memory allocation}
\begin{itemize}
  \item What happens when you call \mintinline{fortran}|allocate(A(1:N))|?
  \pause
  \item Allocating memory does not necessarily allocate memory!
  \item OS tends to use a \emph{first touch policy}.
  \item Memory is allowed when it's first used, a page at a time.
  \item In a NUMA architecture, this means the first thread to touch the array causes creation of space in the memory closest to that thread (core).
\end{itemize}
\end{frame}

%-------------------------------------------------------------------------------
\subsection{First touch}
\begin{frame}[fragile]
\frametitle{Taking advantage of first touch}
\begin{itemize}
  \item Parallelising your data initialisation routine might mean your main loops go faster!
\end{itemize}


\begin{minted}[fontsize=\small,linenos,frame=single]{fortran}
! Allocate and initialise vectors
allocate(A(N), B(N), C(N))
!$omp parallel do
do i = 1, N
  A(i) = 1.0
  B(i) = 2.0
  C(i) = 0.0
end do
!$omp end parallel do

! Vector add
!$omp parallel do
do i = 1, N
  C(i) = A(i) + B(i)
end do
!$omp end parallel do
\end{minted}

\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}
\frametitle{NUMA-aware}
\begin{itemize}
  \item Parallelise your initialisation routines the same way you parallelise the main loops.
  \item This means each thread touches the same data in initialisation and compute.
  \item Should reduce the number of remote memory accesses needed and improve run times.
  \item But, OS is allowed to move threads around cores, and between sockets.
  \item This will mess up your NUMA aware code!
\end{itemize}
\end{frame}

%-------------------------------------------------------------------------------
\section{Thread affinity}
\begin{frame}
\frametitle{Pinning threads}
\begin{itemize}
  \item OpenMP gives you the controls to pin threads to specific cores.
  \item Exposed as \emph{places} and \emph{thread pinning policy} to those places.
  \item By default there is one place consisting of all the cores.
  \item Use the \mintinline{bash}|OMP_PROC_BIND| environment variable to set pinning for all \mintinline{fortran}|parallel| regions.
  \item Can use the \mintinline{bash}|proc_bind| clause for control of specific regions, but advise against this.
\end{itemize}
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}
\frametitle{OMP\_PROC\_BIND}
\begin{itemize}
  \item \mintinline{bash}|OMP_PROC_BIND=false|: The default; threads may move!
  \item \mintinline{bash}|OMP_PROC_BIND=true|: Threads won't move, and follow \mintinline{fortran}|proc_bind| clauses or else the implementation default pinning.
  \item \mintinline{bash}|OMP_PROC_BIND=master|: Threads pinned to same place as master thread.
  \item \mintinline{bash}|OMP_PROC_BIND=close|: Threads are assigned to places close to the master thread.
  If \mintinline{bash}|OMP_NUM_THREADS.eq.ncores|: thread 0 will pin to core 0; thread 1 will pin to core 1; etc
  \item \mintinline{bash}|OMP_PROC_BIND=spread|: Threads are assigned to places ``sparsely''.
  If \mintinline{bash}|OMP_NUM_THREADS.eq.ncores|: thread 0 will pin to socket 0 core 0; thread 1 will pin to socket 1 core 0; thread 2 will pin to socket 0 core 1; etc.
\end{itemize}
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}
\frametitle{Places}
\begin{itemize}
  \item The affinity defines how threads are assigned to places.
  \item Places allow you to divide up the hardware resource, so that threads can be assigned to them.
  \item Default: one place with all cores.
  \item Use \mintinline{bash}|OMP_PLACES| environment variable to control.
  \item \mintinline{bash}|OMP_PLACES=thread|: each place is a single hardware thread.
  \item \mintinline{bash}|OMP_PLACES=cores|: each place is a single core (containing one or more hardware threads).
  \item \mintinline{bash}|OMP_PLACES=sockets|: each place contains the cores of a single socket.
  \item Can also use list notation: \mintinline{bash}|OMP_PLACES="{0:4},{4:4},{8:4},{12:4}"|
\end{itemize}
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}
\frametitle{Thread pinning summary}
\begin{itemize}
  \item In general, going to want to just use \mintinline{bash}|OMP_PROC_BIND=true|.
  \item Sometimes \mintinline{bash}|spread| or \mintinline{bash}|close| gets better performance.
  \item Most effective with a NUMA-aware implementation.
  \item But must be careful with MPI+OpenMP pinning: more on this later\dots
\end{itemize}
\end{frame}

%-------------------------------------------------------------------------------
\section{Hybrid MPI and OpenMP}
\begin{frame}
\frametitle{Why combine MPI+OpenMP}
\begin{itemize}
  \item Larger fewer messages
  \item Fewer MPI ranks
  \item Parallelise other dimension not decomposed with MPI
\end{itemize}
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{MPI programs}
What happens when you run an MPI program?
\begin{minted}{bash}
mpirun -np 16 ./a.out
\end{minted}

\begin{itemize}
  \item 16 processes are spawned on one (or more) nodes according to the hostname list file given by the queuing system.
    \begin{itemize}
      \item E.g. with PBS (\mintinline{bash}|qsub|, etc.) set by \mintinline{bash}|$PBS_NODEFILE|.
    \end{itemize}
  \item There is no reason why these processes have to be serial:
  \begin{itemize}
    \item Each MPI rank could spawn OpenMP threads and run in parallel.
    \item Each MPI rank could use a GPU.
  \end{itemize}
\end{itemize}

\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{Compiling OpenMP and MPI code}
\begin{itemize}
  \item Remember building MPI code just uses the wrapper commands.
  \item Just pass in the OpenMP flag as usual:
    \begin{itemize}
      \item GNU: \mintinline{bash}|mpif90| -fopenmp
      \item Intel: \mintinline{bash}|mpiifort| -qopenmp
      \item Cray: \mintinline{bash}|ftn|
    \end{itemize}
  \item Set the number of OpenMP threads \emph{per rank}.
  \item E.g 2 MPI ranks, 8 threads per rank:
  \begin{minted}{bash}
  OMP_NUM_THREADS=8 mpirun -np 2 ./a.out
  \end{minted}
\end{itemize}
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{Combining OpenMP and MPI}
\begin{itemize}
  \item MPI assumes that each MPI process does not spawn anything else.
  \item Must initialise MPI differently if using threads!
  \begin{minted}{fortran}
  call MPI_Init_thread(required, provided, ierr)
  \end{minted}

  \item You specify a required thread support level, and it returns the the level it could support.
  \item A good idea to check \mintinline{fortran}|provided .ge. required|.
\end{itemize}
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}
\frametitle{Thread support levels}
\begin{itemize}
  \item \mintinline{fortran}|MPI_THREAD_SINGLE| \\
  Only one thread will execute (no threads allowed).

  \item \mintinline{fortran}|MPI_THREAD_FUNNELED| \\
  May spawn threads, but only the original process may call MPI routines: the one that called \mintinline{fortran}|MPI_Init|.

  \item \mintinline{fortran}|MPI_THREAD_SERIALIZED| \\
  May spawn threads and any thread can make MPI calls, but only one at a time. \emph{Your} responsibility to synchronise.

  \item \mintinline{fortran}|MPI_THREAD_MULTIPLE| \\
  May spawn threads and any thread can make MPI calls. The MPI library has to deal with being called in parallel.
\end{itemize}

\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{Example: MPI\_THREAD\_FUNNELED}
Only the original process is allowed to call MPI routines.
\begin{minted}[frame=single]{fortran}
!$omp parallel
... ! Parallel work
!$omp end parallel
call MPI_Sendrecv()
\end{minted}
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{MPI\_THREAD\_SERIALIZED}
The threads are allowed to call MPI, but you must program in synchronisation to ensure only one thread calls MPI at a time.
\begin{minted}[frame=single]{fortran}
!$omp parallel
  ... ! Parallel work
  !$omp critical
  call MPI_Sendrecv()
  !$omp end critical
!$omp end parallel
\end{minted}
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{MPI\_THREAD\_MULTIPLE}
Any thread can call MPI whenever it likes. The \mintinline{fortran}|MPI_THREAD_MULTIPLE| guarantees the MPI library will be OK with this.
\begin{minted}[frame=single]{fortran}
!$omp parallel
  ... ! Parallel work
  call MPI_Sendrecv()
!$omp end parallel
\end{minted}
\end{frame}

%-------------------------------------------------------------------------------
\subsection{Hybrid thread pinning}
\begin{frame}
\frametitle{Thread pinning}
\begin{itemize}
  \item Need to be very careful how MPI ranks and OpenMP threads are mapped to the physical hardware.
  \item Imagine 2 dual-socket nodes: 4 sockets with (say) 16 cores per socket.
  \item Launch 64 MPI ranks: 1 per core.
    \begin{itemize}
      \item This is flat MPI.
      \item Launching OpenMP threads will over-allocate threads compared to hardware resource.
      \item Warning: things will slow down.
    \end{itemize}
  \item Launch 4 MPI ranks (one per socket).
    \begin{itemize}
      \item Leaves 16 cores per MPI rank for OpenMP threads to run on.
      \item But need to make sure threads go to the right places!
      \item Often close interaction with the queuing system --- system dependant behaviour.
    \end{itemize}
\end{itemize}
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{Example: default placement}
Example MPI rank placement with standard PBS setup.

\begin{minted}{bash}
mpirun -np 4 ./a.out
\end{minted}

\begin{center}
\begin{adjustbox}{max width={\textwidth}}
\begin{tikzpicture}

  \foreach \loc in {0, 3, 7, 10} {
  \foreach \i in {0,...,1} {
    \foreach \j in {0,...,1} {
      \draw (\loc+\i,\j) rectangle (\loc+\i+1,\j+1);
    }
  }
  }

  \draw[dashed] (-0.5,-0.5) rectangle (5.5,2.5);
  \draw[dashed] (6.5,-0.5) rectangle (12.5,2.5);

  \foreach \i in {0,...,1} {
    \foreach \j in {0,...,1} {
      \draw<2->[fill=red] (3+\i+.5,\j+.5) circle (0.4cm);
    }
  }
\end{tikzpicture}
\end{adjustbox}
\end{center}
\onslide<2->{
 All ranks placed on the second socket of the first node.
}
\end{frame}

%-------------------------------------------------------------------------------

\begin{frame}[fragile]
\frametitle{Example: pin MPI to one core per socket}
\begin{itemize}
  \item Tell the OS and MPI runtime to pin each MPI to the first core in each socket.
  \item For OpenMPI:
  \begin{minted}{bash}
  export OMP_NUM_THREADS=4
  mpirun -np 4 --npersocket 1 ./a.out
  \end{minted}
  \item Where do the threads go?
\end{itemize}


\begin{center}
\begin{adjustbox}{max width={\textwidth}}
\begin{tikzpicture}

  \foreach \loc in {0, 3, 7, 10} {
  \foreach \i in {0,...,1} {
    \foreach \j in {0,...,1} {
      \draw (\loc+\i,\j) rectangle (\loc+\i+1,\j+1);
    }
  }
  }

  \draw[dashed] (-0.5,-0.5) rectangle (5.5,2.5);
  \draw[dashed] (6.5,-0.5) rectangle (12.5,2.5);

  \foreach \i in {0, 3, 7, 10} {
    \draw[fill=red] (\i+0.5,1.5) circle (0.4cm);
    \foreach \j in {0.2, 0.4, 0.6, 0.8} {
      \draw<2->[->,line width=.5mm] (\i+\j,1.8) -- (\i+\j, 1.3);
    }
  }
\end{tikzpicture}
\end{adjustbox}
\end{center}

\onslide<2->{
Threads spawned inherit their parent's binding, which was one core.

Use \mintinline{bash}|--report-bindings| flag to see what's being pinned where.

}

\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}
\frametitle{Example: pin MPI to socket}
\begin{itemize}
  \item Pin each MPI to cores on a socket, then OpenMP threads can spawn across the socket.
  \item OpenMPI gives three ways to do this:
  \begin{itemize}
    \item \mintinline{bash}|--bind-to-socket|
    \item \mintinline{bash}|--bind-to-core --cpus-per-proc 8|
    \item \mintinline{bash}|--map-by socket:PE=8| (v1.10 and up)
  \end{itemize}
\end{itemize}
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}
\frametitle{Pinning with Intel and Cray}
\begin{itemize}
  \item Intel MPI will need different flags and environment variables, but tends to do the right thing by default.
  \item Cray MPI (MVAPICH) can be controlled using \mintinline{bash}|aprun|.
  \item Cray MPI with the Intel compiler needs a different set of \mintinline{bash}|aprun| flags.
\end{itemize}
\end{frame}

%-------------------------------------------------------------------------------
\section{Exercise}
\begin{frame}
\frametitle{Exercise}
\begin{itemize}
  \item Make your parallel 5-point stencil code NUMA aware.
  \item Parallelise the initialisation routine.
  \item Experiment with thread affinity.
\end{itemize}
\end{frame}

%-------------------------------------------------------------------------------

\end{document}

