\documentclass{beamer}

\input{preamble.tex}

\title{OpenMP for Computational Scientists}

\begin{document}

\frame{\titlepage}

%-------------------------------------------------------------------------------
\begin{frame}
\frametitle{Thanks}
Thanks go to the following authors, whose own OpenMP tutorials have inspired this one:
\begin{itemize}
  \item Tim Mattson (Intel)
  \item Simon McIntosh-Smith and the HPC team (UoBristol)
  \item Gethin Williams (UoBristol)
  \item and many others
\end{itemize}
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}
\frametitle{Exercises}
\begin{itemize}
\item This is a hands-on course!
\item Exercises will be set for you to try programming OpenMP yourselves.
\item Sample solutions also provided.
\item All the exercises will be in Fortran.
\end{itemize}

Download code (and slides) from:
\url{https://github.com/UoB-HPC/openmp-for-cs}
\end{frame}

%-------------------------------------------------------------------------------
\section{Outline}
\begin{frame}
\frametitle{Course Outline}
Organised as 6 sessions teaching OpenMP plus top-tips for getting good performance.
\begin{enumerate}
  \item OpenMP overview
  \item Data sharing and reductions
  \item Code optimisations
  \item Vectoriation, NUMA and MPI interop
  \item GPU programming with OpenMP
  \item Tasks and Tools
\end{enumerate}
\end{frame}
%-------------------------------------------------------------------------------

\begin{frame}
\frametitle{The first exercise}
\begin{itemize}
  \item At the end of this session, you will be able to parallelise a (simple) 5-point stencil code using OpenMP!
  \item The other sessions provide you with details you might need for real world codes.
\end{itemize}
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}
\frametitle{What is OpenMP?}

A collection of compiler directives, library routines, and environment variables for parallelism for shared memory parallel programs.

\begin{itemize}
  \item Create and manage parallel programs while permitting portability.
  \item User-directed parallelization.
\end{itemize}

A \emph{specfication} of annotations you can make to your program in order to make it parallel.

\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{Syntax}
\begin{itemize}
\item OpenMP mostly formed of \emph{compiler directives}\\
  \begin{minted}{fortran}
  !$omp construct [clause [clause]...]
  \end{minted}
  These tell the compiler to insert some extra code on your behalf.

\item Compiler directives usually apply to a \emph{structured block} of statements.
Limited scoping in Fortran means often need \emph{end} directives.
  \begin{minted}{fortran}
  !$omp construct
  ... ! lines of Fortran code
  !$omp end construct
  \end{minted}

\item Library API calls
  \begin{minted}{fortran}
  use omp_lib
  call omp_...()
  \end{minted}

\end{itemize}
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{Building with OpenMP}

Turn on OpenMP in the compiler:
\begin{minted}{bash}
gfortran *.f90 -fopenmp # GNU
ifort *.f90 -qopenmp    # Intel
ftn *.f90               # Cray (on by default)
pgf90 *.f90 -mp         # PGI
\end{minted}

To also use the API calls within the code, use the library:
\begin{minted}{fortran}
USE omp_lib
\end{minted}

\begin{alertblock}{Note}
No need to include the library if only using the compiler directives.
The library only gets you the API calls.
\end{alertblock}
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}
\frametitle{Shared memory}
OpenMP is for shared memory programming: all threads have access to a shared address space.

A typical HPC node consisting of 2 multi-core CPUs.
\begin{center}
\begin{tikzpicture}
  % Draw 4 cores for socket 0
  \draw (0,0) rectangle (1,1);
  \draw (0.5,0.5) node {C0};
  \draw (1,0) rectangle (2,1);
  \draw (1.5,0.5) node {C1};
  \draw (0,1) rectangle (1,2);
  \draw (0.5,1.5) node {C2};
  \draw (1,1) rectangle (2,2);
  \draw (1.5,1.5) node {C3};
  \draw (1,-0.5) node {Socket 0};

  % Draw 4 cores for socket 1
  \draw (3,0) rectangle (4,1);
  \draw (3.5,0.5) node {C0};
  \draw (4,0) rectangle (5,1);
  \draw (4.5,0.5) node {C1};
  \draw (3,1) rectangle (4,2);
  \draw (3.5,1.5) node {C2};
  \draw (4,1) rectangle (5,2);
  \draw (4.5,1.5) node {C3};
  \draw (4,-0.5) node {Socket 1};

  % Draw large memory
  \draw (-0.5,3) rectangle (5.5,4);
  \draw (2.5,3.5) node {Memory};

  % Connect sockets to memory
  \draw (1,2) -- (1,3);
  \draw (4,2) -- (4,3);
  \draw[dashed] (2,1) -- (3,1); % QPI

  % Show memory shared
  \pause
  \draw[fill=red] (0.5,3.2) rectangle (1,3.7);
  \draw[->] (0.5,1.8) -- (0.7,3.2);
  \draw[->] (0.7,3.2) -- (4.5,0.8);

\end{tikzpicture}
\end{center}
\emph{All} threads (running on a core) access the same memory.

Different to MPI, where processes cannot see memory of another without explicit communication.

\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}
\frametitle{Fork-join model}
Serial/sequential execution:
\begin{center}
\begin{tikzpicture}
  \draw[->] (0,0) -- (8,0);
\end{tikzpicture}
\end{center}

In a \emph{fork-join} model, code starts serial, \emph{forks} a \emph{team} of threads then \emph{joins} them back to serial execution.
\begin{center}
\begin{tikzpicture}
  \draw (0,0) -- (1,0);

  % Fork
  \draw (1,0) -- (2,1.5);
  \draw (1,0) -- (2,0.5);
  \draw (1,0) -- (2,-0.5);
  \draw (1,0) -- (2,-1.5);
  \draw (1,-1) node {Fork};

  % Run in parallel
  \draw (2,1.5)  -- (5,1.5);
  \draw (2,0.5)  -- (5,0.5);
  \draw (2,-0.5) -- (5,-0.5);
  \draw (2,-1.5) -- (5,-1.5);
  \draw (3.5,0) node {Parallel execution};

  % Join
  \draw (5,1.5)  -- (6,0);
  \draw (5,0.5)  -- (6,0);
  \draw (5,-0.5) -- (6,0);
  \draw (5,-1.5) -- (6,0);
  \draw (6,-1) node {Join};

  % Serial end
  \draw[->] (6,0) -- (8,0);
\end{tikzpicture}
\end{center}

Nested threads are allowed, where a thread forks it's own team of threads.
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{Creating OpenMP threads}
\begin{minted}[frame=single, linenos]{fortran}
program hello

!$omp parallel
  print *, "Hello"
!$omp end parallel

end program hello
\end{minted}

Threads \emph{redundantly} execute code in the block.

Each thread will output \mintinline{bash}|Hello|.

Threads are synchronised at the end of the parallel region.

\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{Pthreads}

\begin{minted}[fontsize=\small, linenos, frame=single]{fortran}
program hello
  use fpthread
  intger :: i, err
  integer :: N = 4
  type(fpthread_t) :: tid(N)

  do i = 1, N
    call fpthread_create(tid(i), NULL, run, NULL, err)
  end do
  do i = 1, N
    call fpthread_join(tid(i), NULL, err)
  end do

  subroutine run
    print *, "Hello"
  end subroutine run
end program hello
\end{minted}

\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}
\frametitle{OpenMP and Pthreads}
\begin{itemize}
  \item Pthreads is very error prone and verbose.
  \item The OpenMP \mintinline{fortran}|!$omp parallel| abstracts this away.
  \item The compiler directive inserts this extra code for your behalf.
  \item Pthreads requires wrapping up my parallel work in subroutines.
    \begin{itemize}
      \item Kernels are a useful abstraction used in many programming models.
    \end{itemize}
  \item OpenMP much more convenient for \emph{incrementally} adding parallelism to your code.
\end{itemize}
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{Setting number of threads}
Need to be able to set the number of threads to launch.

OpenMP has 3 ways to do this:
\begin{itemize}
  \item Environment variables
  \begin{minted}{bash}
  OMP_NUM_THREADS=16
  \end{minted}

  \item API calls
  \begin{minted}{fortran}
  call omp_set_num_threads(16)
  \end{minted}

  \item Clauses
  \begin{minted}{fortran}
  !$omp parallel num_threads(16)
  !$omp end parallel
  \end{minted}
\end{itemize}

In general it's better to use environment variables to set this as gives you more flexibility at runtime.
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{Thread API calls}
Most parallel programs are written in a SPMD style: {\bf S}ingle {\bf P}rogram, {\bf M}ultiple {\bf D}ata.
\begin{itemize}
  \item MPI has a SPMD model.
  \item Threads run the same code, and use their ID to work out which data to operate on.
\end{itemize}

The OpenMP API gives you calls to determine thread information when \emph{inside} a parallel region:
\begin{itemize}
  \item Get number of threads
    \begin{minted}{fortran}
    nthreads = omp_get_num_threads()
    \end{minted}

  \item Get thread ID
    \begin{minted}{fortran}
    tid = omp_get_thread_num()
    \end{minted}

\end{itemize}
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{Vector add}
Walkthrough parallelising vector addition using OpenMP.

\begin{minted}[fontsize=\small,linenos,frame=single]{fortran}
program vecadd
  integer :: N = 1024  ! Length of array
  ! Arrays
  real(kind=8), allocatable, dimension(:) :: A, B, C
  integer :: i         ! Loop counter

  ! Allocate and initilise vectors
  allocate(A(N), B(N), C(N))
  A = 1.0; B = 2.0; C = 0.0

  ! Vector add
  do i = 1, N
    C(i) = A(i) + B(i)
  end do

  deallocate(A,B,C)
end program vecadd
\end{minted}
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{Vector add: Step 1}
Add parallel region around work
\begin{minted}[frame=single]{fortran}
  !$omp parallel
  do i = 1, N
    C(i) = A(i) + B(i)
  end do
  !$omp end parallel
\end{minted}
Every thread will now do the entire vector addition --- redundantly!
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{Vector add: Step 2}
Get thread IDs
\begin{minted}[frame=single]{fortran}
  integer :: tid, nthreads

  !$omp parallel
  tid = omp_get_thread_num()
  nthreads = omp_get_num_threads()

  do i = 1, N
    C(i) = A(i) + B(i)
  end do
  !$omp end parallel
\end{minted}

\pause
\begin{alertblock}{Incorrect behaviour at runtime}
What's the problem here?
\end{alertblock}
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{Vector add: Step 2, take 2}

\begin{itemize}
  \item In OpenMP, all variables are \emph{shared} between threads.
  \item But each thread needs own copy of \mintinline{fortran}|tid|.
  \item Solution: use the \mintinline{fortran}|private| clause on the \mintinline{fortran}|parallel| region.
  \item This gives each thread it's own unique copy in memory for the variable.
\end{itemize}

\begin{minted}[frame=single]{fortran}
  integer :: tid, nthreads

  !$omp parallel private(tid)
  tid = omp_get_thread_num()
  nthreads = omp_get_num_threads()

  do i = 1, N
    C(i) = A(i) + B(i)
  end do
  !$omp end parallel
\end{minted}
Much more information about data sharing clausing in next session.
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{Vector add: Step 3}
Finally, share the iteration space between the threads.
\begin{minted}[frame=single]{fortran}
  integer :: tid, nthreads

  !$omp parallel private(tid)
  tid = omp_get_thread_num()
  nthreads = omp_get_num_threads()

  do i = 1+(tid*N/nthreads), (tid+1)*N/nthreads
    C(i) = A(i) + B(i)
  end do
  !$omp end parallel
\end{minted}
\begin{block}{Remember}
Thread IDs are numbered from 0 in OpenMP.
Be careful with your index calculation.
\end{block}
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{Worksharing}

\begin{itemize}
\item The SPMD approach requires lots of bookkeeping.
\item Common pattern of splitting loop iterations between threads.
\item OpenMP has worksharing constructs to help with this.
\item Used within a parallel region.
\item Loop iterator is made \mintinline{fortran}|private| by default: no need for data sharing clause.
\end{itemize}

\begin{minted}[frame=single]{fortran}
  !$omp parallel
  !$omp do
  do i = 1, N
    C(i) = A(i) + B(i)
  end do
  !$omp end do
  !$omp end parallel
\end{minted}

Implicit synchronisation point at the \mintinline{fortran}|!$omp end do|.

\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{Combined worksharing directives}
Generally it's convenient to combine the directives:
\begin{minted}[frame=single]{fortran}
!$omp parallel do
do i = 1, N
  ... ! loop body
end do
!$omp end parallel do
\end{minted}

\begin{itemize}
  \item This starts a parallel region, forking some threads.
  \item Each thread then gets a portion of the iteration space and computes the loop body in parallel.
  \item Implicit synchronisation point at the \mintinline{fortran}|end do|.
  \item Threads finally join again; later code executes sequentually.
\end{itemize}
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}
\frametitle{Vector add code}
The vector add codes are available in the repository for you to look at:
\begin{itemize}
  \item Serial: \mintinline{bash}|vadd.f90|
  \item SPMD: \mintinline{bash}|vadd_spmd.f90|
  \item Worksharing: \mintinline{bash}|vadd_paralleldo.f90|
\end{itemize}
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}
\frametitle{Taking stock}
\begin{itemize}
  \item Now seen how to parallelise a program using OpenMP.
  \item Shown the MPI-style SPMD approach for dividing work.
  \item OpenMP worksharing constructs make this easier.
\end{itemize}

The rest of this session:
\begin{itemize}
  \item Expands on the worksharing constructs.
  \item The first example for you to try.
\end{itemize}

Next time: the rest of the OpenMP common core.
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{The Schedule clause}
\begin{itemize}
  \item The worksharing clauses use default rules for assiging iterations to threads.
  \item Can use the \mintinline{fortran}|schedule| clause to specify the distribution.
  \item General format:
    \begin{minted}{fortran}
      !$omp parallel do schedule(...)
    \end{minted}
\end{itemize}
Next slides go through the options, using the following loop as an example:
\begin{minted}[frame=single]{fortran}
!$omp parallel do num_threads(4)
do i = 1, 100
  ... ! loop body
end do
!$omp end parallel do
\end{minted}

\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{Static schedule}
\begin{minted}{fortran}
schedule(static)
schedule(static,16)
\end{minted}

\begin{itemize}
\item Static schedule divides iterations into chunks and assigns chunks to threads in round-robin.
\item If no chunk size specified, iteration space divided roughly equally.
\end{itemize}
For our example loop:
\begin{columns}
\begin{column}{0.5\textwidth}
  \mintinline{fortran}|schedule(static)|
  \begin{tabular}{cc}
  \toprule
  Thread ID & Iterations \\
  \midrule
  0 &  1--25 \\
  1 & 26--50 \\
  2 & 51--75 \\
  3 & 76--100 \\
  \bottomrule
  \end{tabular}
\end{column}

\begin{column}{0.5\textwidth}
  \mintinline{fortran}|schedule(static,16)|
  \begin{tabular}{cc}
  \toprule
  Thread ID & Iterations \\
  \midrule
  0 &  1--16, 65--80 \\
  1 & 17--32, 81--96 \\
  2 & 33--48, 97--100 \\
  3 & 49--64 \\
  \bottomrule
  \end{tabular}
\end{column}
\end{columns}

\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{Dynamic schedule}
\begin{minted}{fortran}
schedule(dynamic)
schedule(dynamic,16)
\end{minted}

\begin{itemize}
  \item Iteration space is divided into chunks according to chunk size.
  \item If no chunk size specified, default size is one.
  \item Each thread requests and executes a chunk, until no more chunks remain.
  \item Useful for unbalanced work-loads if some threads complete work faster.
\end{itemize}

For our example with a chunk size of 16:
\begin{itemize}
  \item The iteration space is split into chunk of 16 (the last chunk may be smaller).
  \item Each threads gets one chunk, then requests a new chunk to work on.
\end{itemize}

\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{Guided schedule}
\begin{minted}{fortran}
schedule(guided)
schedule(guided,16)
\end{minted}

\begin{itemize}
  \item Similar to assignment to dynamic, except the chunk size decreases over time.
  \item Granularity of work chunks gets finer over time.
  \item If no chunk size is specified, the default size is one.
  \item Useful to try to mitigate overheads of a \mintinline{fortran}|dynamic| schedule by starting with large chunks of work.
\end{itemize}

For our example with a chunk size of 16:
\begin{itemize}
  \item Each thread gets a chunk of 16 to work on.
  \item Each thread requests a new chunk, which might be smaller than 16.
\end{itemize}

\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{Other schedules}
\begin{minted}{fortran}
schedule(auto)
\end{minted}
\begin{itemize}
  \item Let the compiler or runtime choose the schedule.
\end{itemize}

\vfill

\begin{minted}{fortran}
schedule(runtime)
\end{minted}
\begin{itemize}
  \item Get the schedule from the \mintinline{bash}|OMP_SCHEDULE| environment variable.
\end{itemize}

\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{Nested loops}
\begin{itemize}
  \item Often have tightly nested loops in your code.
  \item E.g. 2D grid code, every cell is independent.
  \item OpenMP worksharing would only parallelise over first loop with each thread performing inner loop serially.
  \item Use the \mintinline{fortran}|collapse(...)| clause to combine iteration spaces.
  \item OpenMP then workshares the combined iteration space.
\end{itemize}

\begin{minted}[frame=single]{fortran}
!$omp parallel do collapse(2)
do i = 1, N
  do j = 1, N
    ... ! loop body
  end do
end do
!$omp end parallel do
\end{minted}
All $N^2$ iterations are shared between threads.

\end{frame}


%-------------------------------------------------------------------------------
\begin{frame}
\frametitle{Nested loops}
\begin{block}{Performance note}
Collapsing loops may subtly effect the compilers knowledge about alignment and effect vectoriation.
More on this when we talk about vectorisation in another session.
\end{block}

\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{The nowait clause}
\begin{itemize}
  \item May have series of loops in your code which are independent.
  \item Threads must wait/synchronise at the end of the loop \mintinline{fortran}|!$omp end do|.
  \item But it might be possible to delay this synchronisation using the \mintinline{fortran}|nowait| clause.
  \item When a thread finishes the first loop, it starts on the next loop.
\end{itemize}

\begin{minted}[fontsize=\small, linenos, frame=single]{fortran}
!$omp parallel
!$omp do nowait
do i = 1, N
  A(i) = i
end do
!$omp end do       ! No barrier!
!$omp do
do i = 1, N
  B(i) = i
!$omp end do       ! Implicit barrier
!$omp end parallel ! Implicit barrier
\end{minted}
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}
\frametitle{Synchronisation}
A number of ways to synchronise the threads in OpenMP:
\begin{columns}
\begin{column}{0.5\textwidth}
\begin{itemize}
  \item Barriers
  \item Critical
  \item Atomics
  \item Locks
\end{itemize}
\end{column}
\begin{column}{0.5\textwidth}
\begin{itemize}
  \item Ordered
  \item Single
  \item Master
  \item Flush
\end{itemize}
\end{column}
\end{columns}

\vfill

\begin{itemize}
  \item Will look at Critical and Atomic in Session 2.
  \item Ordered, Single and Master in Session 6.
  \item Won't formally cover Flush and Locks --- advanced stuff with esoteric use cases.
\end{itemize}

Quickly cover barriers now.

\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{Barriers}
A barrier simply synchronises threads in a parallel region.

\begin{minted}[frame=single,linenos]{fortran}
!$omp parallel private(tid)

tid = omp_get_thread_num()
A(tid) = big_work1(tid)

!$omp barrier

B(tid) = big_work2(A, tid)

!$omp end parallel
\end{minted}

\begin{itemize}
  \item Running in parallel, need to compute \mintinline{fortran}|A(:)| before computing \mintinline{fortran}|B(:)|.
  \item The barrier ensures all threads wait between these statements.
  \item Must ensure all threads encounter the barrier.
\end{itemize}

\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}
\frametitle{Nested threads}
sections, single, tasks?
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}
\frametitle{5-point stencil example}
First exercise: parallelise a simple 5-point stencil code using OpenMP.

\begin{center}
\begin{tikzpicture}
\draw[step=1cm,gray,very thin] (-1.9,-1.9) grid (2.9,2.9);
\draw[fill=black] (0.5,0.5) circle (0.1cm);
\draw[fill=black] (-0.5,0.5) circle (0.1cm);
\draw[fill=black] (1.5,0.5) circle (0.1cm);
\draw[fill=black] (0.5,1.5) circle (0.1cm);
\draw[fill=black] (0.5,-0.5) circle (0.1cm);
\draw (-0.5,0.5) -- (1.5,0.5);
\draw (0.5,-0.5) -- (0.5,1.5);
\end{tikzpicture}
\end{center}

Value in every cell is set to the average of it's neighbours.
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{5-point stencil example}
Take \mintinline{bash}|stencil.f90| and parallelise it using OpenMP:
\begin{enumerate}
  \item Using a SPMD style
  \item Using the OpenMP worksharing clauses
\end{enumerate}

The main loop to parallelise is:
\begin{minted}[breaklines]{fortran}
do i = 1, nx
  do j = 1, ny
    Anew(i,j) = (A(i-1,j) + A(i+1,j) + A(i,j-1) + A(i,j+1)) / 4.0
  end do
end do
\end{minted}
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}
\frametitle{Summary}
This section introduced the OpenMP programming model:
\begin{itemize}
  \item Creating parallel regions: \mintinline{fortran}|!$omp parallel|/\mintinline{fortran}|!$omp end parallel|
  \item Getting thread IDs: \mintinline{fortran}|omp_get_thread_num()|/\mintinline{fortran}|omp_get_num_threads()|
  \item Worksharing constructs: \mintinline{fortran}|!$omp do|/\mintinline{fortran}|!$omp end do|
  \item Anything else?
\end{itemize}
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}
\frametitle{Resources}
OpenMP secficiation (not for the faint hearted)
Online tutorials
Books
\end{frame}

%-------------------------------------------------------------------------------

\end{document}
